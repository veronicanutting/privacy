{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603835c3",
   "metadata": {},
   "source": [
    "## Privacy Policy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5665180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat as ts\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cffd364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/veronicanutting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/veronicanutting/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372776fc",
   "metadata": {},
   "source": [
    "Note: I considered and worked a bit with web-scraping tools like Beautiful Soup and PDF scraping tools like PyPDF2 to try to find broad ways to systemize my analysis. I found that the Privacy Policies varied significantly in structure and organization, so I decided to start with a less universalizable approach first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a2e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given text file, return text as long string\n",
    "def readText(filename):\n",
    "    with open(filename) as file:\n",
    "        text = file.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b311fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of lines in text not stripped of '\\n'\n",
    "def countLines(text):\n",
    "    return text.count('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f28f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates various readabillity metrics\n",
    "# Link to explanations: https://pypi.org/project/textstat/\n",
    "def calculateComplexity(text):\n",
    "    \n",
    "    # The Flesch Reading Ease formula\n",
    "    # 90-100 is Very Easy, 0-29 is Very Confusing\n",
    "    # Max score is 121.22, no limit on how low score can be\n",
    "    print('Flesch:',ts.flesch_reading_ease(text))\n",
    "    \n",
    "    print('Dale-Chall:',ts.dale_chall_readability_score(text))\n",
    "    \n",
    "    # Based upon all the above tests, returns the estimated school grade level required to understand the text.\n",
    "    print('Consensus:',ts.text_standard(text, float_output=True))\n",
    "    print('Reading Time:',ts.reading_time(text, ms_per_char=14.69)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658378bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given cleaned list of words, return counts\n",
    "def findCommonWords(words_list):\n",
    "    return Counter(cleaned_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd0d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLongestWord(words_list):\n",
    "    words_list = set(words_list)\n",
    "    sorted_words = sorted(words_list, key = len)\n",
    "    \n",
    "    max_word_length = len(sorted_words[-1])\n",
    "    max_words = []\n",
    "    \n",
    "    for i in range(len(sorted_words) - 1, -1, -1) :\n",
    "        if len(sorted_words[i])==max_word_length:\n",
    "            max_words.append(sorted_words[i])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Print longest (last) word\n",
    "    return max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a018c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return list of cleaned words\n",
    "def cleanWords(text):\n",
    "    \n",
    "    # 1. Remove punctuation, change to lowercase\n",
    "    words = (re.sub(\"[^a-zA-Z]\", \" \", text)).lower().split()\n",
    "\n",
    "    # 2. Remove common, stop words\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    # 3. Lemmatize words to remove plurals for analysis\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        lemmatized_words.append(word)\n",
    "        \n",
    "    return words,lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d5da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {'BBC':'bbc',\n",
    "            'Harvard Book Store':'bookstore',\n",
    "            'Wired':'conde',\n",
    "            'Ebay':'ebay',\n",
    "            'YouTube':'google',\n",
    "            'Kayak':'kayak',\n",
    "            'StackOverflow':'stack',\n",
    "            'Sweetgreen':'sweetgreen',\n",
    "            'Target':'target',\n",
    "            'Zappos':'zappos',\n",
    "            'Zoom':'zoom'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2598f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC\n",
      "Linecount: 378\n",
      "Flesch: 58.15\n",
      "Dale-Chall: 6.85\n",
      "Consensus: 13.0\n",
      "Reading Time: 4.079666666666666\n",
      "Wordcount: 3730\n",
      "Unique words: 666\n",
      "Cleaned Wordcount: 1726\n",
      "Cleaned Unique words: 530\n",
      "Longest word: ['personalisation']\n",
      "Most common words: [('information', 83), ('bbc', 60), ('service', 56), ('use', 43), ('u', 40), ('might', 35), ('personal', 34), ('cooky', 24), ('account', 24), ('like', 21)]\n",
      "\n",
      "Ebay\n",
      "Linecount: 371\n",
      "Flesch: 19.98\n",
      "Dale-Chall: 7.59\n",
      "Consensus: 21.0\n",
      "Reading Time: 10.991333333333333\n",
      "Wordcount: 8357\n",
      "Unique words: 1239\n",
      "Cleaned Wordcount: 4818\n",
      "Cleaned Unique words: 1046\n",
      "Longest word: ['networkadvertising']\n",
      "Most common words: [('data', 235), ('personal', 113), ('service', 109), ('ebay', 99), ('user', 77), ('information', 66), ('use', 61), ('may', 48), ('e', 46), ('third', 46)]\n",
      "\n",
      "Harvard Book Store\n",
      "Linecount: 8\n",
      "Flesch: 41.19\n",
      "Dale-Chall: 9.48\n",
      "Consensus: 13.0\n",
      "Reading Time: 0.22133333333333333\n",
      "Wordcount: 176\n",
      "Unique words: 107\n",
      "Cleaned Wordcount: 89\n",
      "Cleaned Unique words: 77\n",
      "Longest word: ['responsibility']\n",
      "Most common words: [('information', 5), ('privacy', 4), ('right', 2), ('visit', 2), ('harvard', 2), ('com', 2), ('collect', 2), ('message', 1), ('customer', 1), ('concerning', 1)]\n",
      "\n",
      "Kayak\n",
      "Linecount: 403\n",
      "Flesch: 21.2\n",
      "Dale-Chall: 7.05\n",
      "Consensus: 19.0\n",
      "Reading Time: 13.874500000000001\n",
      "Wordcount: 10589\n",
      "Unique words: 1234\n",
      "Cleaned Wordcount: 5783\n",
      "Cleaned Unique words: 1032\n",
      "Longest word: ['youronlinechoices']\n",
      "Most common words: [('information', 310), ('service', 174), ('may', 99), ('use', 97), ('personal', 84), ('party', 83), ('third', 81), ('travel', 73), ('partner', 69), ('account', 67)]\n",
      "\n",
      "StackOverflow\n",
      "Linecount: 240\n",
      "Flesch: 14.4\n",
      "Dale-Chall: 8.0\n",
      "Consensus: 21.0\n",
      "Reading Time: 6.946\n",
      "Wordcount: 5307\n",
      "Unique words: 927\n",
      "Cleaned Wordcount: 2922\n",
      "Cleaned Unique words: 778\n",
      "Longest word: ['confidentiality', 'acknowledgement']\n",
      "Most common words: [('information', 148), ('stack', 85), ('overflow', 81), ('privacy', 62), ('may', 61), ('service', 60), ('personal', 56), ('product', 45), ('use', 41), ('collect', 40)]\n",
      "\n",
      "Sweetgreen\n",
      "Linecount: 158\n",
      "Flesch: 31.35\n",
      "Dale-Chall: 7.36\n",
      "Consensus: 15.0\n",
      "Reading Time: 6.288333333333333\n",
      "Wordcount: 4729\n",
      "Unique words: 876\n",
      "Cleaned Wordcount: 2634\n",
      "Cleaned Unique words: 722\n",
      "Longest word: ['networkadvertising']\n",
      "Most common words: [('information', 128), ('service', 75), ('personal', 65), ('may', 63), ('data', 59), ('privacy', 42), ('use', 42), ('u', 38), ('party', 33), ('online', 32)]\n",
      "\n",
      "Target\n",
      "Linecount: 453\n",
      "Flesch: 18.86\n",
      "Dale-Chall: 7.71\n",
      "Consensus: 19.0\n",
      "Reading Time: 8.455833333333334\n",
      "Wordcount: 6406\n",
      "Unique words: 1030\n",
      "Cleaned Wordcount: 3798\n",
      "Cleaned Unique words: 857\n",
      "Longest word: ['pseudoephedrine']\n",
      "Most common words: [('information', 173), ('target', 138), ('service', 64), ('may', 55), ('request', 48), ('e', 46), ('mobile', 45), ('g', 45), ('use', 42), ('device', 42)]\n",
      "\n",
      "Wired\n",
      "Linecount: 449\n",
      "Flesch: 8.27\n",
      "Dale-Chall: 7.89\n",
      "Consensus: 29.0\n",
      "Reading Time: 10.130500000000001\n",
      "Wordcount: 7276\n",
      "Unique words: 1187\n",
      "Cleaned Wordcount: 4341\n",
      "Cleaned Unique words: 996\n",
      "Longest word: ['globalprivacycontrol']\n",
      "Most common words: [('information', 175), ('personal', 123), ('service', 111), ('provider', 73), ('may', 72), ('data', 62), ('party', 56), ('privacy', 51), ('product', 49), ('policy', 47)]\n",
      "\n",
      "YouTube\n",
      "Linecount: 391\n",
      "Flesch: 25.7\n",
      "Dale-Chall: 7.77\n",
      "Consensus: 21.0\n",
      "Reading Time: 6.672166666666667\n",
      "Wordcount: 5168\n",
      "Unique words: 902\n",
      "Cleaned Wordcount: 2951\n",
      "Cleaned Unique words: 731\n",
      "Longest word: ['troubleshooting', 'confidentiality', 'personalization']\n",
      "Most common words: [('information', 165), ('google', 136), ('service', 99), ('use', 55), ('account', 48), ('privacy', 42), ('like', 41), ('collect', 38), ('ad', 34), ('data', 34)]\n",
      "\n",
      "Zappos\n",
      "Linecount: 47\n",
      "Flesch: 29.93\n",
      "Dale-Chall: 8.32\n",
      "Consensus: 17.0\n",
      "Reading Time: 2.3858333333333333\n",
      "Wordcount: 1821\n",
      "Unique words: 539\n",
      "Cleaned Wordcount: 1022\n",
      "Cleaned Unique words: 437\n",
      "Longest word: ['responsibility', 'recommendation']\n",
      "Most common words: [('information', 54), ('zappos', 38), ('com', 28), ('customer', 18), ('use', 17), ('privacy', 14), ('notice', 14), ('u', 14), ('may', 14), ('account', 14)]\n",
      "\n",
      "Zoom\n",
      "Linecount: 180\n",
      "Flesch: 4.65\n",
      "Dale-Chall: 8.66\n",
      "Consensus: 27.0\n",
      "Reading Time: 6.281166666666667\n",
      "Wordcount: 4718\n",
      "Unique words: 861\n",
      "Cleaned Wordcount: 2742\n",
      "Cleaned Unique words: 706\n",
      "Longest word: ['lionheartsquared', 'disproportionate']\n",
      "Most common words: [('zoom', 88), ('data', 87), ('information', 84), ('account', 82), ('personal', 73), ('product', 50), ('meeting', 47), ('may', 45), ('owner', 36), ('user', 34)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze each policy\n",
    "for p in sorted(policies.keys()):\n",
    "    print(p)\n",
    "    text = readText(\"policies/\"+str(policies[p])+\".txt\")\n",
    "    print('Linecount:',countLines(text))\n",
    "    calculateComplexity(text)\n",
    "    words,cleaned_words = cleanWords(text)\n",
    "    print('Wordcount:',len(words))\n",
    "    print('Unique words:',len(set(words)))\n",
    "    print('Cleaned Wordcount:',len(cleaned_words))\n",
    "    print('Cleaned Unique words:',len(set(cleaned_words)))\n",
    "    print('Longest word:',findLongestWord(cleaned_words))\n",
    "    print('Most common words:',findCommonWords(cleaned_words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1670f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
